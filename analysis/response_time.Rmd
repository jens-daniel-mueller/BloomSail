---
title: "Response time correction of Contros HydroC pCO~2~ data"
author: "Jens Daniel Müller"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---


```{r global_options, include = FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r packages}
library(tidyverse)
library(seacarb)
library(data.table)
library(broom)
library(lubridate)
```

# Sensitivity considerations

A change in DIC of 1 µmol kg^-1^ corresponds to a change in pCO~2~ of around 1 µatm, in the Central Baltic Sea at a pCO~2~ of around 100 µatm (summertime conditions).

```{r sensitivity_estimate, fig.cap="pCO~2~ sensitivity to changes in DIC."}

df <- data.frame(cbind(
  (c(1720)),
  (c(7))))

Tem <- seq(5,25,5)
pCO2<-seq(50,500,20)

df<-merge(df, Tem)
names(df) <- c("AT", "S", "Tem")  

df<-merge(df, pCO2)
names(df) <- c("AT", "S", "Tem", "pCO2")  

df<-data.table(df)
df$AT<-df$AT*1e-6

df$DIC<-carb(flag=24, var1=df$pCO2, var2=df$AT, S=df$S, T=df$Tem, k1k2="m10", kf="dg", pHscale="T")[,16]
df$pCO2.corr<-carb(flag=15, var1=df$AT, var2=df$DIC, S=df$S, T=df$Tem, k1k2="m10", kf="dg", pHscale="T")[,9]

df$pCO2.2<-df$pCO2.corr + 25
df$DIC.2<-carb(flag=24, var1=df$pCO2.2, var2=df$AT, S=df$S, T=df$Tem, k1k2="m10", kf="dg", pHscale="T")[,16]


df$ratio<-(df$pCO2.2-df$pCO2.corr)/(df$DIC.2*1e6-df$DIC*1e6)

df %>% 
  ggplot(aes(pCO2, ratio, col=as.factor(Tem)))+
  geom_line()+
  scale_color_viridis_d(option = "C",name="Tem [°C]")+
  labs(x=expression(pCO[2]~(µatm)), y=expression(Delta~pCO[2]~"/"~Delta~DIC~(µatm~µmol^{-1}~kg)))+
  scale_y_continuous(limits = c(0,8), breaks = seq(0,10,1))

rm(df, Tem, pCO2)

```

# Response time determination

## HydroC sensor settings

The sensor was first run with a low power pump (1W). Later - and for most parts of the expedition - with a stronger (8W) pump. Pumps were switched between recordings (data file: SD_datafile_20180718_170417CO2-0618-001.txt):  

* 2018-07-17;13:08:34
* 2018-07-17;13:08:35

Logging frequency for all measurement modes (Measure, Zero, Flush) was increased in two steps, It was:  

10 sec for all recordings including SD_datafile_20180714_073641CO2-0618-001.txt  

2 sec after change in SD_datafile_20180717_121052CO2-0618-001.txt at:  

* 2018-07-14;07:52:02
* 2018-07-14;07:52:12
* 2018-07-14;07:52:14

1 sec after change in SD_datafile_20180718_170417CO2-0618-001 at:  

* 2018-07-17;12:27:25
* 2018-07-17;12:27:27
* 2018-07-17;12:27:28

## Model fitting

Response times were determined by fitting a nonlinear least-squares model with the `nls` function as described [here](http://douglas-watson.github.io/post/2018-09_exponential_curve_fitting/) by Douglas Watson.

* Flush period length: variable
* Flush period restricted to equilibration phase, avoiding initial gas mixing effects occuring at the start of each Flush period
* only completed Flush periods (duration > 500 sec) included


```{r response_determination_data_preparation}

df <- read_csv(here::here("data/_merged_data_files",
                          "BloomSail_CTD_HydroC_Contros_clean.csv"),
               col_types = cols(ID = col_character(),
                                pCO2_analog = col_double(),
                                pCO2 = col_double(),
                                Zero = col_factor(),
                                Flush = col_factor(),
                                Zero_ID = col_integer(),
                                duration = col_double(),
                                mixing = col_character()))

df <- df %>%
  select(date_time, ID, dep, tem, Flush, pCO2, Zero_ID, duration, mixing)

df <- df %>%
  filter(Flush == 1, mixing == "equilibration")

df <- df %>% 
  group_by(Zero_ID) %>% 
  mutate(duration = duration- min(duration),
         max_duration = max(duration)) %>% 
  ungroup() %>% 
  filter(max_duration >= 500) %>% 
  select(-max_duration)

```

An example plot for a `nls` model fitted to pCO~2~ observations during a Flush phase is shown below.

```{r response_determination_example_plot, fig.cap="Example response time determination by non-linear least squares fit to the pCO~2~ recovery signal after zeroing. The vertical line indicates the determined response time tau. The horizontal line indicates 63% of the difference between start and final fitted pCO~2~."}

i <- 95

df_ID <- df %>%
  filter(Zero_ID == i, duration <= 300)

fit <-
df_ID %>%
  nls(pCO2 ~ SSasymp(duration, yf, y0, log_alpha), data = .)

tau <- as.numeric(exp(-tidy(fit)[3,2]))
pCO2_end <- as.numeric(tidy(fit)[1,2])
pCO2_start <- as.numeric(tidy(fit)[2,2])
dpCO2 = pCO2_end - pCO2_start
mean_abs_resid <- mean(abs(resid(fit)))

augment(fit) %>%
  ggplot(aes(duration, pCO2))+
  geom_point()+
  geom_line(aes(y = .fitted))+
  geom_vline(xintercept = tau)+
  geom_hline(yintercept = pCO2_start + 0.63 *(dpCO2))+
  labs(y=expression(pCO[2]~(µatm)), x="Duration of Flush period (s)")

rm(df_ID, fit, i, tau, dpCO2, pCO2_end, pCO2_start)
```



```{r set_flush_duration_intervals}
duration_intervals <- seq(150,500,50)
```


As there was some speculation about the dependence of determined response times ($\tau$) on the chosen duration of the fit interval, the response time $\tau$ was determined for all zeroings and for total durations of:  

`r duration_intervals` secs


```{r all_response_times, include=TRUE, eval=FALSE}

# Plot all individual Flush periods with exponential fit ----------------------

pdf(file=here::here("output/Plots/response_time",
    "RT_determination.pdf"), onefile = TRUE, width = 7, height = 4)

for (i in unique(df$Zero_ID)) {
  for (max_duration in duration_intervals) {
    
    df_ID <- df %>%
      filter(Zero_ID == i, duration <= max_duration)
    
    fit <- 
      try(
      df_ID %>%
          nls(pCO2 ~ SSasymp(duration, yf, y0, log_alpha), data = .),
      TRUE)
    
    if (class(fit) == "nls"){
    
      tau <- as.numeric(exp(-tidy(fit)[3,2]))
      pCO2_end <- as.numeric(tidy(fit)[1,2])
      pCO2_start <- as.numeric(tidy(fit)[2,2])
      dpCO2 = pCO2_end - pCO2_start
      mean_abs_resid <- mean(abs(resid(fit))/pCO2_end)*100
      
      temp <- as_tibble(bind_cols(Zero_ID = i, duration = max_duration, tau = tau, resid = mean_abs_resid))
      
      if (exists("tau_df")){tau_df <- bind_rows(tau_df, temp)}
        else {tau_df <- temp}
      
      if (mean_abs_resid > 1){warn <- "orange"}
        else {warn <- "black"}
      
      print(
      augment(fit) %>%
        ggplot(aes(duration, pCO2))+
        geom_point(col = warn)+
        geom_line(aes(y = .fitted))+
        geom_vline(xintercept = tau)+
        geom_hline(yintercept = pCO2_start + 0.63 *(dpCO2))+
        labs(y=expression(pCO[2]~(µatm)), x="Duration of Flush period (s)",
             title = paste("Zero_ID: ", i,
                           "Tau: ", round(tau,1),
                           "Mean absolute residual (%): ", round(mean_abs_resid, 2)))+
        xlim(0,600)
      )
      
    }
    
    
    else {
    
      temp <- as_tibble(bind_cols(Zero_ID = i, duration = max_duration, tau = NaN, resid = NaN))
      
      if (exists("tau_df")){tau_df <- bind_rows(tau_df, temp)}
        else {tau_df <- temp}
      
      print(
      df_ID %>%
        ggplot(aes(duration, pCO2))+
        geom_point(col="red")+
        labs(y=expression(pCO[2]~(µatm)), x="Duration of Flush period (s)",
             title = paste("Zero_ID: ", i,
                           "nls model failed"))+
        xlim(0,600)
      )
      
    }
  }
}

dev.off()

rm(df_ID, fit, i, tau, dpCO2, pCO2_end, pCO2_start, temp, max_duration, mean_abs_resid, warn)

tau_df %>% 
  write_csv(here::here("data/_summarized_data_files", "Tina_V_HydroC_response_times_all.csv"))

# Plot individual Flush periods with linearized response variable  --------
# for (i in unique(df$Zero_ID)) {
# 
# #i <- 50
# df_ID <- df %>%
#   filter(Zero_ID == i,
#          mixing == "equilibration")
# 
# mean_pCO2 <- df_ID %>% 
#   slice((n()-4) : n()) %>% 
#   summarise(mean_pCO2 = mean(pCO2))
# 
# df_ID <- full_join(df_ID, mean_pCO2) %>% 
#   mutate(dpCO2 = max(pCO2) - pCO2,
#          ln_dpCO2 = log(dpCO2))
# 
# 
# df_ID %>%
#   ggplot(aes(duration_equi, ln_dpCO2))+
#   geom_point()+
#   geom_smooth(method = "lm")+
#   theme_bw()
# 
# # augment(fit) %>%
# #   ggplot(aes(duration_equi, pCO2))+
# #   geom_point()+
# #   geom_line(aes(y = .fitted))+
# #   geom_vline(xintercept = tau)
# 
# ggsave(here::here("/Plots/TinaV/Sensor/HydroC_diagnostics/Response_time_fits",
#                   paste(i,"_Zero_ID_HydroC_RT_linear.jpg", sep="")),
#          width = 10, height = 4)
# }

```

A pdf with plots of all individual response time fits can be accessed [here](https://github.com/jens-daniel-mueller/BloomSail/tree/master/output){target="_blank"}


```{r thresholds_for_tau_and_fit_residual, include=FALSE}

tau_df <- read_csv(here::here("data/_summarized_data_files", "Tina_V_HydroC_response_times_all.csv"))

# define subsetting parameters
resid_limit <- 1

# subset determined tau values by residual threshold
tau_resid <- tau_df %>% 
  group_by(Zero_ID) %>% 
  mutate(resid_max = max(resid, na.rm = TRUE)) %>% 
  filter(resid_max < resid_limit) %>% 
  select(-resid_max) %>% 
  ungroup()

tau_resid_out <- tau_df %>% 
  group_by(Zero_ID) %>% 
  mutate(resid_max = max(resid, na.rm = TRUE)) %>% 
  filter(resid_max > resid_limit) %>% 
  select(-resid_max) %>% 
  ungroup()

# Flush periods where model failure occured
tau_df %>% 
  filter(is.na(resid)) %>% 
  group_by(Zero_ID) %>% 
  summarise(n()) %>% 
  ungroup()

# Flush periods removed due to residual criterion
tau_resid_out %>% 
  group_by(Zero_ID) %>% 
  summarise(n()) %>% 
  ungroup()

# mean tau of first RT determination
tau_resid %>% 
  filter(Zero_ID == 2) %>% 
  summarise(tau = mean(tau))

# mean tau of all RT determinations before pump switch, except first
tau_resid %>% 
  filter(Zero_ID != 2, Zero_ID <= 20) %>% 
  summarise(tau = mean(tau))

tau_resid <- tau_resid %>% 
  filter(Zero_ID != 2)

# calculate some metrics for the subsetting
n_Zero_IDs <- tau_df %>% 
  group_by(Zero_ID) %>% 
  n_groups()
n_duration_intervals <- length(duration_intervals)
n_tau_max <- n_Zero_IDs * length(duration_intervals)
n_tau_total <- nrow(tau_df %>% filter(!is.na(resid)))
n_tau_resid <- nrow(tau_resid)

```


## Outcome

### General cosiderations

Estimated $\tau$ values were only taken into account when stable environmental pCO~2~ levels were present. Absence of stable environmental pCO~2~ was assumed when the mean absolute fit residual was above `r resid_limit` % of the final equilibrium pCO~2~. If one model fit (irrespective the chosen fit interval length) of a particular flush period did not match that criterion, the flush period was ignored. Usually, fits with the higher duration did not meet this criterion. For some unexplained reason the first $\tau$ determination resulted in values about twice as high as all other Flush periods and was therefore removed as an outlier.

Metrics to characterize the fitting procedure include the number of:

* Flush periods: `r n_Zero_IDs`
* Duration intervals: `r n_duration_intervals`
* Exercised response time fits: `r n_tau_max`
* Succesful response times determinations: `r n_tau_total` (`r round(100*n_tau_total/n_tau_max, 1)`)%
* $\tau$'s after removing groups of fits with high absolute fit residual: `r n_tau_resid` (`r round(100*n_tau_resid/n_tau_max, 1)` %)

It should be noted that all failed model fits occured in flush periods where the residual criterion was not meet by at least one other fit (i.e. fitting only failed under unstable conditions).

`r rm(n_Zero_IDs, n_duration_intervals,n_tau_max, n_tau_total, n_tau_resid, n_tau_resid_min_max, mean_abs_resid, tau_resid_out)`

```{r fit_residuals, fig.cap="Histogram of residuals from fit displayed for the investigate durations of the fit interval. Vertical line represents the chosen threshold."}

tau_df %>% 
  ggplot(aes(resid))+
  geom_histogram()+
  facet_wrap(~duration, labeller = label_both)+
  geom_vline(xintercept = resid_limit)+
  labs(x=expression(Mean~absolute~residuals~("%"~of~equilibrium~pCO[2])))

```

```{r tau_for_all_zero_ID, fig.cap="Tau for all Zeroings with color representing the fit interval duration."}
tau_resid %>% 
  ggplot(aes(Zero_ID, tau, col=duration))+
  geom_point()+
  scale_color_viridis_c(name="Duration (sec)")+
  labs(y="Tau (sec)")
```


### Fit interval length

```{r tau_duration_individual, fig.cap="Determined tau values as a function of the fit interval duration, displayed individually for each flush period.", fig.asp=2}

tau_resid %>% 
  group_by(Zero_ID) %>% 
  mutate(d_tau = tau - mean(tau)) %>% 
  ggplot(aes(duration, d_tau))+
  geom_hline(yintercept = 0)+
  geom_smooth()+
  geom_point()+
  facet_wrap(~Zero_ID, ncol = 4, labeller = label_both)+
  labs(x="Duration (sec)", y="Deviation from mean tau (sec)")

```


```{r r tau_duration_mean, fig.cap="Determined tau values as a function of the fit interval duration, pooled for all flush period."}

tau_resid %>% 
  group_by(Zero_ID) %>% 
  mutate(d_tau = tau - mean(tau)) %>% 
  ggplot(aes(duration, d_tau))+
  geom_violin(aes(group=duration))+
  geom_point()+
  labs(x="Duration (sec)", y="Deviation from mean tau (sec)")

```


```{r old_script_response_time_determination, eval=FALSE, include=FALSE}

# Response time fitting ---------------------------------------------------

RT <- df %>% 
  filter(duration <= 300) %>% 
  group_by(Zero_ID) %>% 
  do(fit = nls(pCO2 ~ SSasymp(duration, yf, y0, log_alpha), data = .)) %>% 
  tidy(fit) %>% 
  select(Zero_ID, term, estimate) %>% 
  spread(term, estimate) %>% 
  select(1,2) %>% 
  mutate(tau = exp(-log_alpha))


# Residuals from fit ------------------------------------------------------

augmented <- df %>% 
  group_by(Zero_ID) %>% 
  do(fit = nls(pCO2 ~ SSasymp(duration_equi, yf, y0, log_alpha), data = .)) %>% 
  augment(fit)

# qplot(duration_equi, pCO2_corr, data = augmented, geom = 'point', colour = as.factor(Zero_ID)) +
#   geom_line(aes(y=.fitted))
# 
# qplot(duration_equi, .resid, data = augmented, geom = 'point', colour = as.factor(Zero_ID))

augmented_sum <- augmented %>% 
  group_by(Zero_ID) %>% 
  summarise(mean_resid = mean(abs(.resid)),
            mean_resid_rel = mean(abs(.resid))/max(pCO2),
            max_pCO2_corr = max(pCO2))

# Standard error of tau ---------------------------------------------------

St_Err <- df %>% 
  filter(mixing == "equilibration") %>% 
  group_by(Zero_ID) %>% 
  do(fit = nls(pCO2 ~ SSasymp(duration_equi, yf, y0, log_alpha), data = .)) %>% 
  tidy(fit) %>% 
  select(Zero_ID, term, std.error) %>% 
  spread(term, std.error) %>% 
  select(1,2) %>% 
  rename(tau_st_err = log_alpha)


# Merge RT, mean residuals and St error -----------------------------------

RT <- full_join(RT, augmented_sum)
RT <- full_join(RT, St_Err)

rm(augmented, augmented_sum, St_Err)



# Identify residual threshold ---------------------------------------------

# RT %>% 
#   ggplot(aes(mean_resid_rel, Zero_ID, label=Zero_ID)) +
#   geom_point(shape=21)+
#   scale_fill_viridis_c()+
#   geom_label()

RT %>% 
  filter(mean_resid_rel >= 0.0065)

# RT %>% 
#   filter(mean_resid_rel < 0.0065) %>% 
#   ggplot(aes(Zero_ID, tau, label=round(mean_resid_rel,4))) +
#   geom_label(data=RT, aes(Zero_ID, tau, label=round(mean_resid_rel,4)), col="red") +
#   geom_point(shape=21)+
#   scale_fill_viridis_c()+
#   geom_label()

RT %>% 
  filter(mean_resid_rel < 0.0065) %>% 
  ggplot(aes(Zero_ID, tau))+
  geom_point()



# Mean tau ----------------------------------------------------------------

max(unique(df[df$date_time < ymd_hms("2018-07-17;13:08:34"),]$Zero_ID))
unique(df[df$date_time > ymd_hms("2018-07-17;13:08:34"),]$Zero_ID)

RT %>% 
  filter(mean_resid_rel < 0.0065) %>% 
  mutate(pump_power = if_else(Zero_ID <= 20, "1W", "8W")) %>% 
  group_by(pump_power) %>% 
  summarise(tau = mean(tau))

```

### Mean response time

Finally, the mean response times are:

```{r mean_response_time}


max_Zero_ID <- max(unique(df[df$date_time < ymd_hms("2018-07-17;13:08:34"),]$Zero_ID))
min_Zero_ID <- min(unique(df[df$date_time > ymd_hms("2018-07-17;13:08:34"),]$Zero_ID))

# tau_df %>% 
#   mutate(pump_power = if_else(Zero_ID <= 20, "1W", "8W")) %>% 
#   group_by(pump_power) %>% 
#   summarise(tau = mean(tau, na.rm = TRUE))

tau_resid %>% 
  mutate(pump_power = if_else(Zero_ID <= max_Zero_ID, "1W", "8W")) %>% 
  group_by(pump_power) %>% 
  summarise(tau = mean(tau))

```

# Response time correction

## Pre-smoothing
## Response time correction
## Post-smoothing
# Response time optimization
# Open tasks / questions

- Compare Contros and own response time estimates
- Compare differnt response time correction methods (Bittig vs. Fiedler, Miloshevich, Fietzek)
- Check results from field response time experiment (high zeroing frequency)
