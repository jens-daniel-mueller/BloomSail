---
title: "Data base"
author: "Jens Daniel Müller"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---
```{r global_options, include = FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```


```{r packages}
library(tidyverse)
library(data.table)
library(lubridate)
library(DataExplorer)
library(leaflet)
library(readxl)
library(gsubfn)

```


```{r ggplot_theme, include = FALSE}
theme_set(theme_bw())
```


# Sea-Bird SBE 16 sensor data (ts)

CTD sensor data including recordings from auxiliary pH, O~2~, Chla and pCO~2~ sensors were recorded with a measurement frequency of 15 sec. (In addition, pCO~2~ data were also internally recorded on the Contros HydroC instrument with higher temporal resolution and will later be used for further analysis after merging with CTD data.)

## Read regular profiles and transects


```{r read_ts_profile_transect_regular, eval=FALSE}

#setwd("C:/Mueller_Jens_Data/Research/Projects/BloomSail/data/TinaV/Sensor/Profiles_Transects")
files <- list.files(path = "data/TinaV/Sensor/Profiles_Transects/", pattern = "[.]cnv$")
#file <- files[1]

for (file in files){
  
  start.date <- data.table(read.delim(here::here("data/TinaV/Sensor/Profiles_Transects/", file),
                                      sep="#", nrows = 160))[[78,1]]
  start.date <- substr(start.date, 15, 34)
  start.date <- mdy_hms(start.date, tz="UTC")
  
  tempo <- read.delim(here::here("data/TinaV/Sensor/Profiles_Transects/", file),
                      sep="", skip = 160, header = FALSE)
  tempo <- data.table(tempo[,c(2,3,4,5,6,7,9,11,13)])
  names(tempo) <- c("date", "Dep.S", "Tem.S", "Sal.S", "V_pH", "pH", "Chl", "O2", "pCO2")
  tempo$start.date <- start.date
  tempo$date <- tempo$date + tempo$start.date
  
  tempo$transect.ID <- substr(file, 1, 6)
  tempo$type <- substr(file, 8,8)
  tempo$label <- substr(file, 8,10)
  
  tempo$cast <- "up"
  tempo[date < mean(tempo[Dep.S == max(tempo$Dep.S)]$date)]$cast <- "down"
  
  if (exists("dataset")){
    dataset <- rbind(dataset, tempo)
  }
  
  if (!exists("dataset")){
    dataset <- tempo
  }
  
  rm(start.date)
  rm(tempo)
}


ts <- dataset
rm(dataset, file, files)
```

## Read profiles and transects around Ostergarnsholm

```{r read_ts_profile_transect_Ostergarnsholm, eval=FALSE}

files <- list.files(path = "data/TinaV/Sensor/Ostergarnsholm/", pattern = "[.]cnv$")

for (file in files){
  
  start.date <- data.table(read.delim(here::here("data/TinaV/Sensor/Ostergarnsholm/", file),
                                      sep="#", nrows = 160))[[78,1]]
  start.date <- substr(start.date, 15, 34)
  start.date <- mdy_hms(start.date, tz="UTC")
  
  tempo <- read.delim(here::here("data/TinaV/Sensor/Ostergarnsholm/", file),
                      sep="", skip = 160, header = FALSE)
  tempo <- data.table(tempo[,c(2,3,4,5,6,7,9,11,13)])
  names(tempo) <- c("date", "Dep.S", "Tem.S", "Sal.S", "V_pH", "pH", "Chl", "O2", "pCO2")
  tempo$start.date <- start.date
  tempo$date <- tempo$date + tempo$start.date
  
  tempo$transect.ID <- substr(file, 1, 6)
  tempo$type <- substr(file, 8,8)
  tempo$label <- substr(file, 11,12)
    
  tempo$cast <- "up"
  tempo[date < mean(tempo[Dep.S == max(tempo$Dep.S)]$date)]$cast <- "down"
  
  if (exists("dataset")){
    dataset <- rbind(dataset, tempo)
  }
  
  if (!exists("dataset")){
    dataset <- tempo
  }
  
  rm(start.date)
  rm(tempo)
}


ts_OGB <- dataset
rm(dataset, file, files)

ts_OGB <- ts_OGB %>% 
  mutate(type = if_else(label=="bo", "P", "T"),
         label = if_else(label == "bo", "P14", label),
         label = if_else(label == "in", "T14", label),
         label = if_else(label == "ou", "T15", label))

```

```{r merge_ts_regular_and_Ostergarnsholm, eval=FALSE}
ts <- bind_rows(ts, ts_OGB) %>% 
  arrange(date)

rm(ts_OGB)
```

## EDA raw data

```{r ts_EDA_raw, eval=FALSE}

source("code/eda.R")
eda(ts, "ts-raw")
rm(eda)

```

The output of an automated Exploratory Data Analysis (EDA) performed with the package `DataExplorer` can be accessed here:

[Link to EDA report of CTD raw data](https://jens-daniel-mueller.github.io/BloomSail/EDA_report_ts-raw.html){target="_blank}

## Clean data set

Sensor recordings were cleaned from obviously erroneous readings, by setting suspecious values to NA. 

```{r clean_ts_data, eval=FALSE}

class(ts)
ts <- data.table(ts)

# Profiling data

# Temperature

# ts %>%
#   filter(type == "P") %>%
#   ggplot(aes(Tem.S, Dep.S, col=label, linetype = cast))+
#   geom_line()+
#   scale_y_reverse()+
#   geom_vline(xintercept = c(10, 20))+
#   facet_wrap(~transect.ID)

ts[transect.ID == "180723" & label == "P07" & Dep.S < 2 & cast == "up"]$Tem.S <- NA

# Salinity

# ts %>%
#   filter(type == "P") %>%
#   ggplot(aes(Sal.S, Dep.S, col=label, linetype = cast))+
#   geom_path()+
#   scale_y_reverse()+
#   facet_wrap(~transect.ID)

ts[Sal.S < 6]$Sal.S <- NA

# pH

# ts %>%
#   filter(type == "P") %>%
#   ggplot(aes(pH, Dep.S, col=label, linetype=cast))+
#   geom_path()+
#   scale_y_reverse()+
#   facet_wrap(~transect.ID)
# 
# ts %>%
#   filter(type == "P") %>%
#   ggplot(aes(V_pH, Dep.S, col=label, linetype=cast))+
#   geom_path()+
#   scale_y_reverse()+
#   facet_wrap(~transect.ID)

ts[pH < 7.5]$V_pH <- NA
ts[pH < 7.5]$pH <- NA

ts[transect.ID == "180709" & label == "P03" & Dep.S < 5 & cast == "down"]$pH <- NA
ts[transect.ID == "180709" & label == "P05" & Dep.S < 10 & cast == "down"]$pH <- NA
ts[transect.ID == "180718" & label == "P10" & Dep.S < 3 & cast == "down"]$pH <- NA
ts[transect.ID == "180815" & label == "P03" & Dep.S < 2 & cast == "down"]$pH <- NA
ts[transect.ID == "180820" & label == "P11" & Dep.S < 15 & cast == "down"]$pH <- NA

ts[transect.ID == "180709" & label == "P03" & Dep.S < 5 & cast == "down"]$V_pH <- NA
ts[transect.ID == "180709" & label == "P05" & Dep.S < 10 & cast == "down"]$V_pH <- NA
ts[transect.ID == "180718" & label == "P10" & Dep.S < 3 & cast == "down"]$V_pH <- NA
ts[transect.ID == "180815" & label == "P03" & Dep.S < 2 & cast == "down"]$V_pH <- NA
ts[transect.ID == "180820" & label == "P11" & Dep.S < 15 & cast == "down"]$V_pH <- NA


# pCO2

# ts %>%
#   filter(type == "P") %>%
#   ggplot(aes(pCO2, Dep.S, col=label, linetype = cast))+
#   geom_path()+
#   scale_y_reverse()+
#   facet_wrap(~transect.ID)

ts[transect.ID == "180616"]$pCO2 <- NA

# O2

# ts %>% 
#   filter(type == "P") %>% 
#   ggplot(aes(O2, Dep.S, col=label, linetype = cast))+
#   geom_path()+
#   scale_y_reverse()+
#   facet_wrap(~transect.ID)

# Chlorophyll

# ts %>%
#   filter(type == "P") %>%
#   ggplot(aes(Chl, Dep.S, col=label, linetype = cast))+
#   geom_path()+
#   scale_y_reverse()+
#   facet_wrap(~transect.ID)

ts[Chl > 100]$Chl <- NA


#### Surface transect data

# ts %>%
#   filter(type == "T") %>%
#   ggplot(aes(date, Dep.S, col=label))+
#   geom_point()+
#   scale_y_reverse()+
#   facet_wrap(~transect.ID, scales = "free_x")
# 
# ts %>%
#   filter(type == "T") %>%
#   ggplot(aes(date, Tem.S, col=label))+
#   geom_point()+
#   facet_wrap(~transect.ID, scales = "free_x")
# 
# ts %>%
#   filter(type == "T") %>%
#   ggplot(aes(date, Sal.S, col=label))+
#   geom_point()+
#   facet_wrap(~transect.ID, scales = "free_x")
# 
# ts %>%
#   filter(type == "T") %>%
#   ggplot(aes(date, pCO2, col=label))+
#   geom_point()+
#   facet_wrap(~transect.ID, scales = "free_x")
# 
# ts %>%
#   filter(type == "T") %>%
#   ggplot(aes(date, pH, col=label))+
#   geom_point()+
#   facet_wrap(~transect.ID, scales = "free_x")
# 
# ts %>%
#   filter(type == "T") %>%
#   ggplot(aes(date, Chl, col=label))+
#   geom_point()+
#   facet_wrap(~transect.ID, scales = "free_x")

ts[type == "T" & Chl > 10]$Chl <- NA


# ts %>% 
#   filter(type == "T") %>% 
#   ggplot(aes(date, O2, col=label))+
#   geom_point()+
#   facet_wrap(~transect.ID, scales = "free_x")

```

## Write summary file

Relevant columns were selected and renamed, only observations from regular stations (P01-P13) and transects (T01-T13) were selected and summarized data were written to file.

```{r ts_column_subsetting_writing_summary_file, eval=FALSE}

ts <- ts %>% 
  select(date_time=date,
         ID=transect.ID,
         type,
         station=label,
         dep=Dep.S,
         sal=Sal.S,
         tem=Tem.S,
         pCO2_analog=pCO2)

# ts <- ts %>% 
#   filter( !(station %in% c("PX1", "PX2", "TX1", "TX2") ))

ts %>% 
  write_csv(here::here("data/_summarized_data_files", "ts_profiles_transects.csv"))

rm(ts)
```

```{r reopen_CTD_data}
ts <- read_csv(here::here("data/_summarized_data_files", "ts_profiles_transects.csv"),
               col_types = cols(pCO2_analog = col_double()))
```

## EDA clean data

```{r CTD_EDA, eval=FALSE}

source("code/eda.R")
eda(ts, "ts_clean")

rm(eda)
```


The output of an automated Exploratory Data Analysis (EDA) performed with the package `DataExplorer` can be accessed here:

[Link to EDA report of CTD clean data](https://jens-daniel-mueller.github.io/BloomSail/EDA_report_ts_clean.html){target="_blank}

## Overview plots

```{r Temperature_profiles, fig.cap="Temperature profiles by stations. Color refers to the starting date of each cruise.", fig.asp = 1.3}

ts %>%
  arrange(date_time) %>% 
  filter(type == "P", !(station %in% c("PX1", "PX2"))) %>%
  ggplot(aes(tem, dep, col=ymd(ID), group=ID))+
  geom_path()+
  scale_y_reverse()+
  scale_color_viridis_c(trans = "date", name="")+
  labs(x="Temperature (°C)", y="Depth (m)")+
  facet_wrap(~station, labeller = label_both)

```


```{r pCO2_analog_profiles, fig.cap="pCO~2~ (analog signal) profiles by stations. Color refers to the starting date of each cruise.", fig.asp = 1.3}

ts %>%
  arrange(date_time) %>% 
  filter(type == "P", !(station %in% c("PX1", "PX2"))) %>%
  ggplot(aes(pCO2_analog, dep, col=ymd(ID), group=ID))+
  geom_path()+
  scale_y_reverse()+
  scale_color_viridis_c(trans = "date", name="")+
  labs(x="Temperature (°C)", y="Depth (m)")+
  facet_wrap(~station, labeller = label_both)

```


# HydroC CO~2~ data (th)

## Read data

HydroC pCO~2~ data  were provided by KM Contros after applying a drift correction to the raw data, which was based on pre- and post-deployment calibration results. 

```{r read_th_post_processed_by_manufacturer, eval=FALSE}

# Read Contros corrected data file

th <-
  read_csv2(here::here("Data/TinaV/Sensor/HydroC-pCO2/corrected_Contros",
                       "parameter&pCO2s(method 43).txt"),
            col_names = c("date_time", "Zero", "Flush", "p_NDIR",
                          "p_in", "T_control", "T_gas", "%rH_gas",
                          "Signal_raw", "Signal_ref", "T_sensor",
                          "pCO2", "Runtime", "nr.ave")) %>% 
  mutate(date_time = dmy_hms(date_time),
         Flush = as.factor(as.character(Flush)),
         Zero = as.factor(as.character(Zero)))

```

## Deployment identification and subsetting

Individual deployments (periods of observations with less than 30 sec between recordings) were identified and relevant deployment periods were subsetted. This procedure removes only recordings attributable to sensor testing and set-up.

```{r th_deployment_identification_selection, eval=FALSE}

th <- th %>% 
  arrange(date_time) %>% 
  mutate(deployment = cumsum(c(TRUE,diff(date_time)>=30)))

th <- th %>% 
  filter(deployment %in% c(2,6,9,14,17,21,23,27,31,33,34,35,37))

```

## Removal of duplicated time stamps

```{r duplicated_timestamp_removal, eval=FALSE}


# add counter for date_time observations

th <- th %>% 
  add_count(date_time)

# find triplicated time stamp and select only first observation, and merge

th_no_triple <- th %>% 
  filter(n <= 2)

th_triple_clean <- th %>% 
  filter(n > 2) %>% 
  slice(1)

th <- full_join(th_no_triple, th_triple_clean)

rm(list=setdiff(ls(), "th"))


# find duplicated time stamps and shift first by one second backward, and merge

th %>% 
  distinct(date_time)

th <- th %>% 
  select(-n) %>% 
  add_count(date_time)

unique(th$n)

th_no_duplicated <- th %>%
  filter(n == 1)

th_duplicated <- th %>% 
  filter(n == 2)

th_duplicated_first <- th_duplicated %>% 
  group_by(date_time) %>% 
  slice(1) %>% 
  ungroup() %>% 
  mutate(date_time = date_time - 1)

th_duplicated_second <- th_duplicated %>% 
  group_by(date_time) %>% 
  slice(2) %>% 
  ungroup()

th_duplicated_clean <- full_join(th_duplicated_first, th_duplicated_second) %>% 
  arrange(date_time)

th <- full_join(th_no_duplicated, th_duplicated_clean)

th %>% 
  distinct(date_time)

rm(list=setdiff(ls(), "th"))



# find duplicated time stamps and shift first by two seconds forward, and merge

th %>% 
  distinct(date_time)

th <- th %>% 
  select(-n) %>% 
  add_count(date_time)

unique(th$n)

th_no_duplicated <- th %>%
  filter(n == 1)

th_duplicated <- th %>% 
  filter(n == 2)

th_duplicated_first <- th_duplicated %>% 
  group_by(date_time) %>% 
  slice(1) %>% 
  ungroup() %>% 
  mutate(date_time = date_time + 2)

th_duplicated_second <- th_duplicated %>% 
  group_by(date_time) %>% 
  slice(2) %>% 
  ungroup()

th_duplicated_clean <- full_join(th_duplicated_first, th_duplicated_second) %>% 
  arrange(date_time)

th <- full_join(th_no_duplicated, th_duplicated_clean)

th %>% 
  distinct(date_time)

rm(list=setdiff(ls(), "th"))

# remaining duplicates are observations where other observations with a +/- 1 sec timestamp exist
# for those cases, only the first duplicated observation is selected (similar to triplicate treatment)


th %>% 
  distinct(date_time)

th <- th %>% 
  select(-n) %>% 
  add_count(date_time)

unique(th$n)


th_still_no_duplicated <- th %>% 
  filter(n == 1)

th_still_duplicated_first <- th %>% 
  filter(n == 2) %>% 
  group_by(date_time) %>% 
  slice(1)

th <- full_join(th_still_no_duplicated, th_still_duplicated_first)

th %>% 
  distinct(date_time)

rm(list=setdiff(ls(), "th"))

th <- th %>% 
  select(-n)

```

## Flush and Zeroing identification

```{r th_Zeroing_Flush_Identification, eval=FALSE}

# Zeroing ID labelling

th <- th %>% 
  arrange(date_time) %>% 
  group_by(Zero) %>% 
  mutate(Zero_ID = as.factor(cumsum(c(TRUE,diff(date_time)>=30)))) %>% 
  ungroup()

unique(th$Zero_ID)

# Flush: Identification

th <- th %>% 
  mutate(Flush = 0) %>% 
  group_by(Zero, Zero_ID) %>% 
  mutate(start = min(date_time),
         duration = date_time - start,
         Flush = if_else(Zero == 0 & duration < 600, "1", "0")) %>% 
  ungroup()


#  Flush: Identify equilibration and internal gas mixing periods

th <- th %>% 
  mutate(mixing = if_else(duration < 20, "mixing", "equilibration"))



```

## Deployment plots

```{r HydroC_deployment_plots, eval=FALSE}

pdf(file=here::here("output/Plots/read_in",
    "th_deployments.pdf"), onefile = TRUE, width = 7, height = 4)

for (i in unique(th$deployment)) {
  
  #i <- unique(th$deployment)[3]
  
  sub <-  th %>%
      filter(deployment == i)
  start_date <- min(sub$date_time)
  
  print(
    sub %>% 
      ggplot(aes(date_time, pCO2, col=Zero_ID))+
      geom_line()+
      labs(title = paste("Deployment: ",i, "| Start time: ", start_date))
)
  
}

dev.off()

rm(sub, start_date, i)

```

A pdf with pCO~2~ timeseries plots of all deployments can be found here:

[Link to pCO~2~ timeseries plots](https://github.com/jens-daniel-mueller/BloomSail/tree/master/output/Plots/read_in/th_deployments.pdf){target="_blank"}


```{r th_EDA, eval=FALSE}

source("code/eda.R")
eda(th, "th")
rm(eda)

```

The output of an automated Exploratory Data Analysis (EDA) performed with the package `DataExplorer` can be accessed here:

[Link to EDA report of HydroC pCO~2~ data data](https://jens-daniel-mueller.github.io/BloomSail/EDA_report_th.html){target="_blank}

## Write summary file

Summarized pCO~2~ date were written to file.

```{r writing_summary_file, eval=FALSE}

th %>% write_csv(here::here("Data/_summarized_data_files",
                            "th_full.csv"))

th %>% 
  select(date_time, Zero, Flush, pCO2_corr = pCO2, deployment, Zero_ID, duration, mixing) %>% 
  write_csv(here::here("Data/_summarized_data_files",
                       "th.csv"))

rm(th)

```


# Bottle data / discrete samples (tb)

Discrete samples were collected with a Niskin bottle and analysed for C[T] and A[T] at IOW CO~2~ lab.

## Read data

```{r read_bottle_CO2, eval=FALSE}


tb <- read_csv(here::here("Data/TinaV/Bottle/Tracegases", "BloomSail_bottle_CO2_all.csv"),
                   col_types = list("c","c","n","n","n","n","n"))

tb <- tb %>% 
  select(ID=transect.ID,
         station=label,
         dep=Dep,
         sal=Sal,
         CT, AT)


```

## Write summary file

```{r write_bottle_CO2, eval=FALSE}

tb %>% write_csv(here::here("Data/_summarized_data_files",
                            "tb.csv"))
rm(tb)

```



# GPS track (tt)

GPS track data were recorded with a Samsung Galaxy tablet.

## Read data

```{r read_GPS_data, eval=FALSE}

files <- list.files(path = "data/TinaV/Track/GPS_Logger_Track/", pattern = "[.]txt$")


for (file in files){
  
  # if the merged dataset does exist, append to it
  if (exists("dataset")){
    
    tempo<-data.table(read.delim(here::here("data/TinaV/Track/GPS_Logger_Track", file),
                                 sep=",")[,c(2,3,4)])
    names(tempo) <- c("date_time", "lat", "lon")
    tempo$date_time<- ymd_hms(tempo$date, tz="UTC")
    
    dataset<-rbind(dataset, tempo)
    rm(tempo)
  }
  
  # if the merged dataset doesn't exist, create it
  if (!exists("dataset")){
    dataset<-data.table(read.delim(here::here("data/TinaV/Track/GPS_Logger_Track", file),
                                   sep=",")[,c(2,3,4)])
    names(dataset) <- c("date_time", "lat", "lon")
    dataset$date_time<- ymd_hms(dataset$date_time, tz="UTC")
    
  }
}

tt <- dataset
rm(dataset, file, files)

```

## Write summary file

```{r write_track_data, eval=FALSE}

tt %>% 
  write_csv(here::here("data/_summarized_data_files",
                       "tt.csv"))

```


# Finnmaid

pCO~2~ data were recorded on VOS Finnmaid in summer 2018.

## Read data

```{r read_fm_data, eval=FALSE}


### June - August 2018

files <- list.files(path = "data/Finnmaid_2018", pattern = "[.]xls$")
#file <-files[1]

for (file in files){
  
  
  df <- read_excel(here::here("data/Finnmaid_2018", file))
  df <- df[c(1,2,3,12,7,4,15,8,5,17)]
  names(df) <- c("date","Lon","Lat","pCO2","Sal","Tem","cO2","patm", "Teq","xCO2")
  df <- df[-c(1),]
  df$date <- as.POSIXct(as.numeric(df$date)*60*60*24, origin="1899-12-30", tz="GMT")
  df$Lon <- as.numeric(as.character(df$Lon))
  df$Lat <- as.numeric(as.character(df$Lat))
  df$pCO2 <- as.numeric(as.character(df$pCO2))
  df$Sal <- as.numeric(as.character(df$Sal))
  df$Tem <- as.numeric(as.character(df$Tem))
  df$cO2 <- as.numeric(as.character(df$cO2))
  df$patm <- as.numeric(as.character(df$patm))
  df$Teq <- as.numeric(as.character(df$Teq))
  df$xCO2 <- as.numeric(as.character(df$xCO2))
  df <- data.table(df)
  
  df$route <- strapplyc(as.character(file), ".*(.).xls*", simplify = TRUE)
  df$ID <- substr(as.character(file), 3, 10)
  
  if (exists("temp")){
    temp <- rbind (temp, df)
  } else{temp <- df}
  
}


rm(df, files, file)
temp <- temp[pCO2 != 0]


#### Los Gatos data


files <- list.files(path = "data/Finnmaid_2018/LGR", pattern = "[.]xls$")
#file <-files[1]

for (file in files){
  
  
  df <- read_excel(here::here("data/Finnmaid_2018/LGR", file))
  df <- df[c(2,3,4,8,6,5,14,7,15,9)]
  names(df) <- c("date","Lon","Lat","pCO2","Sal","Tem","cO2","patm", "Teq","xCO2")
  df <- df[-c(1),]
  df$date <- dmy_hms(df$date)
  df <- data.table(df)
  
  df$route <- substr(as.character(file), 12, 12)
  df$ID <- substr(as.character(file), 3, 10)
  
  if (exists("temp.LGR")){
    temp.LGR <- rbind (temp.LGR, df)
  } else{temp.LGR <- df}
  
}


```

## Convert O2 concentration units

```{r convert_O2_Los_Gatos, eval=FALSE}

source(here::here("code", "O2stoO2c.R"))

temp.LGR <- temp.LGR %>%
  filter() %>% 
  mutate(cO2 = O2stoO2c(O2sat = cO2, T=Tem, S=Sal, P=3/10, p_atm = 1013.5))


```

```{r merge_Los_Gatos_LICOR_data_files, eval=FALSE}

temp$sensor <- "LICOR"
temp.LGR$sensor <- "LosGatos"

temp <- bind_rows(temp, temp.LGR)

rm(temp.LGR, df, file, files)


# temp$Area <- with(temp,
#                   ifelse(Lon>12 & Lon<12.6, "1.MEB",
#                   ifelse(Lon>13.1 & Lon<14.3, "2.ARK",
#                   ifelse(Lat>57.5 & Lat<58.5 & route %in% c("E", "G"), "4.EGS",
#                   ifelse(Lat>57.3 & Lat<57.5 & route %in% c("E"), "BS",
#                   ifelse(Lat>56.8 & Lat<57.5 & route=="W", "3.WGS",
#                   ifelse(Lat>58.5 & Lat<59 & Lon>20, "5.NGS",
#                   ifelse(Lon>22 & Lon<24, "6.WGF",
#                   ifelse(Lon>24 & Lon<24.5, "7.HGF", "NaN")))))))))



```

## Write summary file

```{r write_fm, eval=FALSE}


temp <-temp[complete.cases(temp[,pCO2]),]

temp %>% 
  write_csv(here::here("Data/_summarized_data_files",
                       "fm.csv"))

```



# Interactive map

```{r map}

fm <- read_csv(here::here("Data/_summarized_data_files",
                       "fm.csv"))

fm_sub <- fm %>%
  arrange(date) %>% 
  slice(which(row_number() %% 20 == 1))

tt <- read_csv(here::here("Data/_summarized_data_files", "tt.csv"))

tt_sub <- tt %>%
  slice(which(row_number() %% 20 == 1))

rm(tt, fm)

leaflet() %>% 
  setView(lng = 20, lat = 57.3, zoom = 8) %>%
  addLayersControl(baseGroups = c("Ocean Basemap",
                                  "Satellite"),
                   overlayGroups = c("BloomSail", "Finnmaid"),
                   options = layersControlOptions(collapsed = FALSE),
                   position = 'topright') %>% 
  addProviderTiles("Esri.WorldImagery", group = "Satellite") %>%
  addProviderTiles(providers$Esri.OceanBasemap, group = "Ocean Basemap") %>%
  addScaleBar(position = 'topright') %>%
  addMeasure(
    primaryLengthUnit = "kilometers",
    secondaryLengthUnit = 'miles', 
    primaryAreaUnit = "sqmeters",
    secondaryAreaUnit="acres", 
    position = 'topleft') %>% 
  addCircles(data = fm_sub, ~Lon, ~Lat,
               color = "white",
               group = "Finnmaid") %>% 
  addPolylines(data = tt_sub, ~lon, ~lat,
               color = "red",
               group = "BloomSail")


```


# Tasks / open questions
